{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GIN Experiment\n",
    "This notebook will implement the evaluation pipeline with the FID calculation using the GIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from util import load_graph_list\n",
    "from util import load_data, load_synth_data, separate_data , load_graph_asS2Vgraph\n",
    "from models.graphcnn import GraphCNN\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(iters_per_epoch, batch_size, model, device, train_graphs, optimizer, epoch):\n",
    "    model.train()\n",
    "\n",
    "    total_iters = iters_per_epoch\n",
    "    pbar = tqdm(range(total_iters), unit='batch')\n",
    "\n",
    "    loss_accum = 0\n",
    "    for pos in pbar:\n",
    "        selected_idx = np.random.permutation(len(train_graphs))[:batch_size]\n",
    "\n",
    "        batch_graph = [train_graphs[idx] for idx in selected_idx]\n",
    "        output = model(batch_graph)\n",
    "\n",
    "        labels = torch.LongTensor([graph.label for graph in batch_graph]).to(device)\n",
    "\n",
    "        #compute loss\n",
    "        loss = criterion(output, labels)\n",
    "\n",
    "        #backprop\n",
    "        if optimizer is not None:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()         \n",
    "            optimizer.step()\n",
    "        \n",
    "\n",
    "        loss = loss.detach().cpu().numpy()\n",
    "        loss_accum += loss\n",
    "\n",
    "        #report\n",
    "        pbar.set_description('epoch: %d' % (epoch))\n",
    "\n",
    "    average_loss = loss_accum/total_iters\n",
    "    print(\"loss training: %f\" % (average_loss))\n",
    "    \n",
    "    return average_loss\n",
    "\n",
    "###pass data to model with minibatch during testing to avoid memory overflow (does not perform backpropagation)\n",
    "def pass_data_iteratively(model, graphs, minibatch_size = 64):\n",
    "    model.eval()\n",
    "    output = []\n",
    "    idx = np.arange(len(graphs))\n",
    "    for i in range(0, len(graphs), minibatch_size):\n",
    "        sampled_idx = idx[i:i+minibatch_size]\n",
    "        if len(sampled_idx) == 0:\n",
    "            continue\n",
    "        output.append(model([graphs[j] for j in sampled_idx]).detach())\n",
    "    return torch.cat(output, 0)\n",
    "\n",
    "def test(model, device, train_graphs, test_graphs, epoch):\n",
    "    model.eval()\n",
    "\n",
    "    output = pass_data_iteratively(model, train_graphs)\n",
    "    pred = output.max(1, keepdim=True)[1]\n",
    "    labels = torch.LongTensor([graph.label for graph in train_graphs]).to(device)\n",
    "    correct = pred.eq(labels.view_as(pred)).sum().cpu().item()\n",
    "    acc_train = correct / float(len(train_graphs))\n",
    "\n",
    "    output = pass_data_iteratively(model, test_graphs)\n",
    "    pred = output.max(1, keepdim=True)[1]\n",
    "    labels = torch.LongTensor([graph.label for graph in test_graphs]).to(device)\n",
    "    correct = pred.eq(labels.view_as(pred)).sum().cpu().item()\n",
    "    acc_test = correct / float(len(test_graphs))\n",
    "\n",
    "    print(\"accuracy train: %f test: %f\" % (acc_train, acc_test))\n",
    "\n",
    "    return acc_train, acc_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = None\n",
    "device = 0 \n",
    "batch_size = 32\n",
    "iters_per_epoch = 50\n",
    "epochs = 60\n",
    "lr = 0.01\n",
    "seed = 0\n",
    "fold_idx = 1\n",
    "num_layers = [5]\n",
    "num_mlp_layers = 3\n",
    "hidden_dims = [64]\n",
    "final_dropout = 0.5\n",
    "graph_pooling_type = \"sum\"\n",
    "neighbor_pooling_type = \"sum\"\n",
    "learn_eps = False\n",
    "degree_as_tag = True\n",
    "filename = \"\"\n",
    "random = 0\n",
    "onehot=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_4342/765707260.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;31m#set up seeds and gpu device\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 6\u001B[0;31m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmanual_seed\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      7\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrandom\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mseed\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[0mdevice\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdevice\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"cuda:\"\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcuda\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mis_available\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdevice\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"cpu\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/graphRL/lib/python3.7/site-packages/torch/random.py\u001B[0m in \u001B[0;36mmanual_seed\u001B[0;34m(seed)\u001B[0m\n\u001B[1;32m     34\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     35\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcuda\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_is_in_bad_fork\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 36\u001B[0;31m         \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcuda\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmanual_seed_all\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mseed\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     37\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     38\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mdefault_generator\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmanual_seed\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mseed\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/graphRL/lib/python3.7/site-packages/torch/cuda/random.py\u001B[0m in \u001B[0;36mmanual_seed_all\u001B[0;34m(seed)\u001B[0m\n\u001B[1;32m    111\u001B[0m             \u001B[0mdefault_generator\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmanual_seed\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mseed\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    112\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 113\u001B[0;31m     \u001B[0m_lazy_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcb\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    114\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    115\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/graphRL/lib/python3.7/site-packages/torch/cuda/__init__.py\u001B[0m in \u001B[0;36m_lazy_call\u001B[0;34m(callable)\u001B[0m\n\u001B[1;32m    112\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0m_lazy_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcallable\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    113\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mis_initialized\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 114\u001B[0;31m         \u001B[0mcallable\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    115\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    116\u001B[0m         \u001B[0;31m# TODO(torch_deploy): this accesses linecache, which attempts to read the\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/graphRL/lib/python3.7/site-packages/torch/cuda/random.py\u001B[0m in \u001B[0;36mcb\u001B[0;34m()\u001B[0m\n\u001B[1;32m    109\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mi\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdevice_count\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    110\u001B[0m             \u001B[0mdefault_generator\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcuda\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdefault_generators\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 111\u001B[0;31m             \u001B[0mdefault_generator\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmanual_seed\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mseed\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    112\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    113\u001B[0m     \u001B[0m_lazy_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcb\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: CUDA error: device-side assert triggered"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "train_acc,test_acc=[],[]\n",
    "\n",
    "#set up seeds and gpu device\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)    \n",
    "device = torch.device(\"cuda:\" + str(device)) if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(0)\n",
    "\n",
    "if dataset != None :\n",
    "    graphs, num_classes = load_data(dataset, degree_as_tag)\n",
    "else :\n",
    "    graphs, num_classes, tagset , lentagset = load_synth_data(True, random, onehot,True)\n",
    "\n",
    "for num_layer,hidden_dim in product(num_layers,hidden_dims):\n",
    "    fold_test_accuracy=[]\n",
    "    fold_train_accuracy=[]\n",
    "    for fold_idx in range(1,2):\n",
    "        train_graphs, test_graphs = separate_data(graphs, seed, fold_idx)\n",
    "\n",
    "        model = GraphCNN(num_layer, num_mlp_layers, train_graphs[0].node_features.shape[1], hidden_dim, num_classes, final_dropout, learn_eps, graph_pooling_type, neighbor_pooling_type,random, device).to(device)\n",
    "\n",
    "        optimizer = optim.Adam(model.parameters(),lr=lr)\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)\n",
    "        \n",
    "        for epoch in range(1, epochs + 1):\n",
    "            scheduler.step()\n",
    "\n",
    "            avg_loss = train(iters_per_epoch,batch_size,model, device, train_graphs, optimizer, epoch)\n",
    "            acc_train, acc_test = test( model, device, train_graphs, test_graphs, epoch)\n",
    "            fold_test_accuracy.append(acc_test)\n",
    "            fold_train_accuracy.append(acc_train)\n",
    "    train_acc.append((np.mean(fold_train_accuracy),num_layer,hidden_dim))\n",
    "    test_acc.append((np.mean(fold_test_accuracy),num_layer,hidden_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pass_data_iteratively(model, graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as PathEffects\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "sns.set_palette('muted')\n",
    "sns.set_context(\"notebook\", font_scale=1.5,\n",
    "                rc={\"lines.linewidth\": 2.5})\n",
    "\n",
    "def fashion_scatter(x, colors):\n",
    "    # choose a color palette with seaborn.\n",
    "    num_classes = len(np.unique(colors))\n",
    "    palette = np.array(sns.color_palette(\"hls\", num_classes))\n",
    "\n",
    "    # create a scatter plot.\n",
    "    f = plt.figure(figsize=(8, 8))\n",
    "    ax = plt.subplot(aspect='equal')\n",
    "    sc = ax.scatter(x[:,0], x[:,1], lw=0, s=40, c=palette[colors.astype(np.int)])\n",
    "    plt.xlim(-25, 25)\n",
    "    plt.ylim(-25, 25)\n",
    "    ax.axis('tight')\n",
    "\n",
    "    # add the labels for each digit corresponding to the label\n",
    "    txts = []\n",
    "\n",
    "    for i in range(num_classes):\n",
    "\n",
    "        # Position of each label at median of data points.\n",
    "\n",
    "        xtext, ytext = np.median(x[colors == i, :], axis=0)\n",
    "        txt = ax.text(xtext, ytext, str(i), fontsize=24)\n",
    "        txt.set_path_effects([\n",
    "            PathEffects.Stroke(linewidth=5, foreground=\"w\"),\n",
    "            PathEffects.Normal()])\n",
    "        txts.append(txt)\n",
    "\n",
    "    return f, ax, sc, txts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=output.cpu()\n",
    "y = np.array([g.label for g in graphs])\n",
    "############################################################\n",
    "from sklearn.manifold import TSNE\n",
    "import time\n",
    "time_start = time.time()\n",
    "\n",
    "fashion_tsne = TSNE(random_state=0).fit_transform(X)\n",
    "fashion_scatter(fashion_tsne, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from scipy import linalg\n",
    "## Coming from https://github.com/mseitzer/pytorch-fid\n",
    "def compute_FID(mu1, mu2, cov1, cov2, eps = 1e-6):\n",
    "    assert mu1.shape == mu2.shape, \\\n",
    "        'Training and test mean vectors have different lengths'\n",
    "    assert cov1.shape == cov2.shape, \\\n",
    "        'Training and test covariances have different dimensions'\n",
    "\n",
    "    diff = mu1 - mu2\n",
    "    # Product might be almost singular\n",
    "    covmean, _ = linalg.sqrtm(cov1.dot(cov2), disp=False)\n",
    "    if not np.isfinite(covmean).all():\n",
    "        msg = ('fid calculation produces singular product; '\n",
    "                'adding %s to diagonal of cov estimates') % eps\n",
    "        print(msg)\n",
    "        offset = np.eye(cov1.shape[0]) * eps\n",
    "        covmean = linalg.sqrtm((cov1 + offset).dot(cov2 + offset))\n",
    "\n",
    "    # Numerical error might give slight imaginary component\n",
    "    if np.iscomplexobj(covmean):\n",
    "        if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n",
    "            m = np.max(np.abs(covmean.imag))\n",
    "            raise ValueError('Imaginary component {}'.format(m))\n",
    "        covmean = covmean.real\n",
    "\n",
    "    tr_covmean = np.trace(covmean)\n",
    "\n",
    "    return (diff.dot(diff) + np.trace(cov1) +\n",
    "            np.trace(cov2) - 2 * tr_covmean)\n",
    "\n",
    "def compute_fid(ref_graph,pred_graph,model):\n",
    "    device = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        embed_graphs_ref = model.get_graph_embed_sum(ref_graph)\n",
    "        embed_graphs_ref=embed_graphs_ref.cpu().detach().numpy()\n",
    "        mu_ref = np.mean(embed_graphs_ref, axis = 0)\n",
    "        cov_ref = np.cov(embed_graphs_ref, rowvar = False)\n",
    "\n",
    "        embed_graphs_pred = model.get_graph_embed_sum(pred_graph)\n",
    "        embed_graphs_pred=embed_graphs_pred.cpu().detach().numpy()\n",
    "        mu_pred = np.mean(embed_graphs_pred, axis = 0)\n",
    "        cov_pred = np.cov(embed_graphs_pred, rowvar = False)\n",
    "\n",
    "    fid = compute_FID(mu_ref,mu_pred,cov_ref,cov_pred)\n",
    "    return fid\n",
    "\n",
    "def test_acc(model, device, train_graphs):\n",
    "    model.eval()\n",
    "\n",
    "    output = pass_data_iteratively(model, train_graphs)\n",
    "    pred = output.max(1, keepdim=True)[1]\n",
    "    labels = torch.LongTensor([graph.label for graph in train_graphs]).to(device)\n",
    "    correct = pred.eq(labels.view_as(pred)).sum().cpu().item()\n",
    "    acc = correct / float(len(train_graphs))\n",
    "\n",
    "\n",
    "    #print(\"accuracy : %f\" % (acc))\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_fid(filename,label,model):\n",
    "    graph_gen = load_graph_list(filename,False)\n",
    "    g_list,_=load_graph_asS2Vgraph(graph_gen,label,random,tagset,lentagset,onehot=onehot)\n",
    "    print(filename+\" fid : \",compute_fid([graphs[i] for i in range(len(graphs)) if graphs[i].label ==label],g_list,model))\n",
    "    print(filename+\" GIN accuracy\",test_acc(model,device,g_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid graph\n",
    "\n",
    "get_fid('../../generated_graphs/grid_GRANMixtureBernoulli_DFS.p',4,model)\n",
    "get_fid('../../generated_graphs/grid_RNN_BFS.p',4,model)\n",
    "get_fid('../../generated_graphs/grid_RNN_MLP_BFS.p',4,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Barabasi\n",
    "get_fid('../../generated_graphs/barabasi_GRANMixtureBernoulli_BFS.p',1,model)\n",
    "get_fid('../../generated_graphs/barabasi_GRANMixtureBernoulli_DFS.p',1,model)\n",
    "get_fid('../../generated_graphs/barabasi_GRANMixtureBernoulli_degree_decent.p',1,model)\n",
    "get_fid('../../generated_graphs/barabasi_GRANMixtureBernoulli_k_core.p',1,model)\n",
    "get_fid('../../generated_graphs/barabasi_GRANMixtureBernoulli_no_order.p',1,model)\n",
    "\n",
    "get_fid('../../generated_graphs/barabasi_RNN_BFS.p',1,model)\n",
    "get_fid('../../generated_graphs/barabasi_RNN_BFSMAX.p',1,model)\n",
    "get_fid('../../generated_graphs/barabasi_RNN_DFS.p',1,model)\n",
    "get_fid('../../generated_graphs/barabasi_RNN_nobfs.p',1,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#watts\n",
    "\n",
    "get_fid('../../generated_graphs/wattsSW_GRANMixtureBernoulli_BFS.p',0,model)\n",
    "get_fid('../../generated_graphs/wattsSW_GRANMixtureBernoulli_DFS.p',0,model)\n",
    "get_fid('../../generated_graphs/wattsSW_GRANMixtureBernoulli_degree_descent.p',0,model)\n",
    "get_fid('../../generated_graphs/wattsSW_GRANMixtureBernoulli_k_core.p',0,model)\n",
    "get_fid('../../generated_graphs/wattsSW_GRANMixtureBernoulli_no_order.p',0,model)\n",
    "\n",
    "get_fid('../../generated_graphs/wattsSW_RNN_BFS.p',0,model)\n",
    "get_fid('../../generated_graphs/wattsSW_RNN_BFSMAX.p',0,model)\n",
    "get_fid('../../generated_graphs/wattsSW_RNN_DFS.p',0,model)\n",
    "get_fid('../../generated_graphs/wattsSW_RNN_nobfs.p',0,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#community2\n",
    "\n",
    "get_fid('../../generated_graphs/community2small_GRANMixtureBernoulli_BFS.p',2,model)\n",
    "get_fid('../../generated_graphs/community2small_GRANMixtureBernoulli_DFS.p',2,model)\n",
    "get_fid('../../generated_graphs/community2small_GRANMixtureBernoulli_degree_decent.p',2,model)\n",
    "get_fid('../../generated_graphs/community2small_GRANMixtureBernoulli_k_core.p',2,model)\n",
    "get_fid('../../generated_graphs/community2small_GRANMixtureBernoulli_degree_accent.p',2,model)\n",
    "\n",
    "get_fid('../../generated_graphs/community2small_RNN_BFS.p',2,model)\n",
    "get_fid('../../generated_graphs/community2small_RNN_BFSMAX.p',2,model)\n",
    "get_fid('../../generated_graphs/community2small_RNN_DFS.p',2,model)\n",
    "get_fid('../../generated_graphs/community2small_RNN_nobfs.p',2,model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_acc(model, device, train_graphs):\n",
    "    model.eval()\n",
    "\n",
    "    output = pass_data_iteratively(model, train_graphs)\n",
    "    pred = output.max(1, keepdim=True)[1]\n",
    "    labels = torch.LongTensor([graph.label for graph in train_graphs]).to(device)\n",
    "    correct = pred.eq(labels.view_as(pred)).sum().cpu().item()\n",
    "    acc = float(correct / float(len(train_graphs)))\n",
    "\n",
    "\n",
    "    print(\"accuracy : %f\" % (acc))\n",
    "\n",
    "    return acc\n",
    "\n",
    "def n_community(c_sizes, p_inter=0.01):\n",
    "    graphs = [nx.gnp_random_graph(c_sizes[i], 0.7, seed=i) for i in range(len(c_sizes))]\n",
    "    G = nx.disjoint_union_all(graphs)\n",
    "    communities = [G.subgraph(c) for c in nx.connected_components(G)]\n",
    "    for i in range(len(communities)):\n",
    "        subG1 = communities[i]\n",
    "        nodes1 = list(subG1.nodes())\n",
    "        for j in range(i + 1, len(communities)):\n",
    "            subG2 = communities[j]\n",
    "            nodes2 = list(subG2.nodes())\n",
    "            has_inter_edge = False\n",
    "            for n1 in nodes1:\n",
    "                for n2 in nodes2:\n",
    "                    if np.random.rand() < p_inter:\n",
    "                        G.add_edge(n1, n2)\n",
    "                        has_inter_edge = True\n",
    "            if not has_inter_edge:\n",
    "                G.add_edge(nodes1[0], nodes2[0])\n",
    "    # print('connected comp: ', len(list(nx.connected_component_subgraphs(G))))\n",
    "    return G\n",
    "\n",
    "test_acc(model,'cuda:0',g_list_pred_rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perturbation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perturb_new(graph_list, p):\n",
    "    ''' Perturb the list of graphs by adding/removing edges.\n",
    "    Args:\n",
    "        p_add: probability of adding edges. If None, estimate it according to graph density,\n",
    "            such that the expected number of added edges is equal to that of deleted edges.\n",
    "        p_del: probability of removing edges\n",
    "    Returns:\n",
    "        A list of graphs that are perturbed from the original graphs\n",
    "    '''\n",
    "    perturbed_graph_list = []\n",
    "    for G_original in graph_list:\n",
    "        G = G_original.copy()\n",
    "        edge_remove_count = 0\n",
    "        for (u, v) in list(G.edges()):\n",
    "            if np.random.rand()<p:\n",
    "                G.remove_edge(u, v)\n",
    "                edge_remove_count += 1\n",
    "        # randomly add the edges back\n",
    "        for i in range(edge_remove_count):\n",
    "            while True:\n",
    "                u = np.random.randint(0, G.number_of_nodes())\n",
    "                v = np.random.randint(0, G.number_of_nodes())\n",
    "                if (not G.has_edge(u,v)) and (u!=v):\n",
    "                    break\n",
    "            G.add_edge(u, v)\n",
    "        perturbed_graph_list.append(G)\n",
    "    return perturbed_graph_list\n",
    "\n",
    "graph_ba_regen=[]\n",
    "for i in range(10,20):\n",
    "    for j in range(10,20):\n",
    "        g=nx.grid_2d_graph(i,j)\n",
    "        graph_ba_regen.append(g)\n",
    "g_list_ref_ba_regen0,_ = load_graph_asS2Vgraph(graph_ba_regen,1,random, tagset , lentagset)\n",
    "g_list_ref_ba_regen10,_ = load_graph_asS2Vgraph(perturb_new(graph_ba_regen,0.1),1,random, tagset , lentagset)\n",
    "g_list_ref_ba_regen20,_ = load_graph_asS2Vgraph(perturb_new(graph_ba_regen,0.2),1,random, tagset , lentagset)\n",
    "g_list_ref_ba_regen30,_ = load_graph_asS2Vgraph(perturb_new(graph_ba_regen,0.3),1,random, tagset , lentagset)\n",
    "g_list_ref_ba_regen40,_ = load_graph_asS2Vgraph(perturb_new(graph_ba_regen,0.4),1,random, tagset , lentagset)\n",
    "g_list_ref_ba_regen50,_ = load_graph_asS2Vgraph(perturb_new(graph_ba_regen,0.5),1,random, tagset , lentagset)\n",
    "g_list_ref_ba_regen60,_ = load_graph_asS2Vgraph(perturb_new(graph_ba_regen,0.6),1,random, tagset , lentagset)\n",
    "g_list_ref_ba_regen70,_ = load_graph_asS2Vgraph(perturb_new(graph_ba_regen,0.7),1,random, tagset , lentagset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "g_list_ref_ba_regen80,_ = load_graph_asS2Vgraph(perturb_new(graph_ba_regen,0.8),1,random, tagset , lentagset)\n",
    "g_list_ref_ba_regen90,_ = load_graph_asS2Vgraph(perturb_new(graph_ba_regen,0.9),1,random, tagset , lentagset)\n",
    "g_list_ref_ba_regen100,_ = load_graph_asS2Vgraph(perturb_new(graph_ba_regen,1.0),1,random, tagset , lentagset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_list_ref_ba_regen70,_ = load_graph_asS2Vgraph(perturb_new(graph_ba_regen,0.7),1,random, tagset , lentagset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fids3=[]\n",
    "fids3.append(compute_fid([graphs[i] for i in range(len(graphs)) if graphs[i].label ==4],g_list_ref_ba_regen0,model))\n",
    "fids3.append(compute_fid([graphs[i] for i in range(len(graphs)) if graphs[i].label ==4],g_list_ref_ba_regen10,model))\n",
    "fids3.append(compute_fid([graphs[i] for i in range(len(graphs)) if graphs[i].label ==4],g_list_ref_ba_regen20,model))\n",
    "fids3.append(compute_fid([graphs[i] for i in range(len(graphs)) if graphs[i].label ==4],g_list_ref_ba_regen30,model))\n",
    "fids3.append(compute_fid([graphs[i] for i in range(len(graphs)) if graphs[i].label ==4],g_list_ref_ba_regen40,model))\n",
    "fids3.append(compute_fid([graphs[i] for i in range(len(graphs)) if graphs[i].label ==4],g_list_ref_ba_regen50,model))\n",
    "fids3.append(compute_fid([graphs[i] for i in range(len(graphs)) if graphs[i].label ==4],g_list_ref_ba_regen70,model))\n",
    "fids3.append(compute_fid([graphs[i] for i in range(len(graphs)) if graphs[i].label ==4],g_list_ref_ba_regen80,model))\n",
    "fids3.append(compute_fid([graphs[i] for i in range(len(graphs)) if graphs[i].label ==4],g_list_ref_ba_regen90,model))\n",
    "fids3.append(compute_fid([graphs[i] for i in range(len(graphs)) if graphs[i].label ==4],g_list_ref_ba_regen100,model))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "perturb = np.arange(0,0.8,0.1)\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.xlabel(\"Noise perturbation\")\n",
    "plt.ylabel(\"Fréchet Distance to the ref graph\")\n",
    "plt.plot(perturb,fids3,label='grid')\n",
    "plt.draw()\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3d0494937803f43eb53306e8569b34e94f10729ff691099c0010227141b076fc"
  },
  "kernelspec": {
   "display_name": "PyCharm (graphRL)",
   "language": "python",
   "name": "pycharm-57b05ec7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}